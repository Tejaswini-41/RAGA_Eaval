
# ğŸŒŸ EmpowerHer - Women's Technology Platform  
### ğŸš€ RAGA_EVAL: Evaluating LLM Responses with Precision

<div align="center">
  <img src="https://img.shields.io/badge/version-1.0.0-blue.svg" />
  <img src="https://img.shields.io/badge/node-18.x-green.svg" />
  <img src="https://img.shields.io/badge/react-19.0.0-blue.svg" />
  <img src="https://img.shields.io/badge/mongodb-atlas-green.svg" />
  <img src="https://img.shields.io/badge/license-MIT-yellow.svg" />
</div>

<p align="center">
  <img width="180" src="https://i.imgur.com/pSOxq3J.png" alt="EmpowerHer Logo" />
</p>

<p align="center"><strong>A platform empowering women in tech through education, mentorship, and community support.</strong></p>

---

## ğŸ“‹ Project Overview

**RAGA_EVAL** is a cutting-edge evaluation tool that enhances and assesses responses from Large Language Models (LLMs) using smart, iterative prompt optimization. By leveraging multiple evaluation dimensions, RAGA_EVAL provides deep insights into model behavior and output quality.

---

## âœ¨ Key Features

- ğŸ”„ **Iterative Prompt Optimization** â€“ Improve prompt quality through 3 automatic feedback loops  
- ğŸ“Š **Comprehensive Metrics** â€“ Evaluate outputs with Relevance, Accuracy, BLEU, ROUGE, and more  
- ğŸš¦ **Rate Limiting Handling** â€“ Smart retry and backoff to manage API quotas  
- ğŸ“ˆ **Performance Visualization** â€“ Track model improvements across iterations  
- ğŸ’¡ **Ground Truth Generation** â€“ Reference answers from Gemini for comparison  
- ğŸ“ **Results Export** â€“ Organized outputs in CSV format for easy access  

---

## ğŸ§  Models & Metrics

### ğŸ’¬ **Models Evaluated**
- **Ground Truth:** Gemini  
- **Evaluation Models:** Mixtral, Llama, Qwen, DeepSeek (via Groq API)  

### ğŸ“ **Evaluation Metrics**
| Metric         | Description                          |
|----------------|--------------------------------------|
| âœ… Relevance    | Response relevance to the query      |
| ğŸ¯ Accuracy     | Factual correctness                 |
| ğŸ“š Groundedness | Rooted in reference material         |
| ğŸ“– Completeness | Information coverage                 |
| ğŸ§  Faithfulness | Logical consistency                  |
| ğŸ§¾ BLEU/ROUGE   | Text similarity & fluency checks     |

---

## ğŸ—‚ Folder Structure

```bash
ğŸ“ RAGA_EVAL/
â”œâ”€â”€ interactive_eval.py       # Evaluation CLI interface
â”œâ”€â”€ questions.json            # Evaluation input prompts
â”œâ”€â”€ evaluation/               # Evaluation logic & metrics
â”‚   â”œâ”€â”€ evaluator.py
â”‚   â”œâ”€â”€ metrics.py
â”‚   â”œâ”€â”€ prompt_improver.py
â”‚   â””â”€â”€ recommendation.py
â”œâ”€â”€ models/                   # Model wrappers & config
â”‚   â”œâ”€â”€ gemini_model.py
â”‚   â”œâ”€â”€ groq_models.py
â”‚   â””â”€â”€ model_factory.py
â”œâ”€â”€ RAGA_Eaval/               # RAG agent backend
â”‚   â””â”€â”€ RAGBasedAgent/
â”‚       â”œâ”€â”€ main.py
â”‚       â”œâ”€â”€ GithubAuth.py
â”‚       â”œâ”€â”€ review_generator.py
â”‚       â””â”€â”€ similarity_query.py
â”œâ”€â”€ responses/                # Raw model outputs
â”œâ”€â”€ improved_responses/       # Responses after enhancement
â”œâ”€â”€ prompts/                  # Updated prompts by iteration
â””â”€â”€ results/                  # CSV summary of metrics
```

---

## ğŸ› ï¸ Installation & Setup

### ğŸ”½ Clone Repository
```bash
git clone https://github.com/Tejaswini-41/RAGA_Eaval.git
cd RAGA_Eval
```

### ğŸ§ª Set Up Virtual Environment
```bash
python -m venv venv
source venv/bin/activate        # macOS/Linux
venv\Scripts\activate           # Windows
```

### ğŸ“¦ Install Dependencies
```bash
pip install -r requirements.txt
```

### ğŸ”‘ Configure Environment Variables

Create a `.env` file:

```env
GITHUB_ACCESS_TOKEN=your_github_token
GEMINI_API_KEY=your_gemini_api_key
GROQ_API_KEY=your_groq_api_key
ANTHROPIC_API_KEY=your_anthropic_api_key
```

---

## ğŸš€ How to Use

1. ğŸ“ **Update `questions.json`** with your queries  
2. ğŸ”§ **Run evaluation script**:
   ```bash
   python interactive_eval.py
   ```

3. ğŸ“‹ **Choose from interactive menu**:
   - Evaluate single question
   - Iterative evaluation (3 rounds)
   - List available questions
   - Exit  

4. ğŸ“Š **Review output**:
   - Terminal comparison view
   - CSV exports (`results/`)
   - Enhanced prompts (`prompts/`)

---

## ğŸ“Š Sample Output

```
âœ… Best model for PR review: llama (Score: 0.625)

ğŸ“Š MODEL COMPARISON (Reference: GEMINI)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Model     | Overall | Relev | Accur | Grnd | Comp | BLEU | ROUGE
llama     | 0.625   | 0.74  | 0.85  | 0.80 | 0.20 | 0.18 | 0.20
deepseek  | 0.589   | 0.77  | 0.90  | 0.60 | 0.20 | 0.04 | 0.17
alibaba   | 0.485   | 0.80  | 0.80  | 0.80 | 0.20 | 0.11 | 0.19
```

---

## ğŸš§ Development Status

| Feature                                      | Status |
|---------------------------------------------|--------|
| Core Evaluation Engine                      | âœ…     |
| Multi-Model Support                         | âœ…     |
| Iterative Prompt Enhancement                | âœ…     |
| Rate Limiting & Retry Logic                 | âœ…     |
| CSV Export of Results                       | âœ…     |
| RAG Agent Integration                       | ğŸ”œ     |

---

## ğŸ”® Roadmap

- ğŸ“¦ Integration with RAG-Based Agents  
- ğŸ“‰ Interactive Web Dashboard  
- ğŸ§  Support for Semantic Evaluation Metrics  
- âš™ï¸ Parallel Processing for Large-Scale Runs  

---

## ğŸ‘¥ Contributors

- ğŸ‘©â€ğŸ’» **Tejaswini Durge**  
- ğŸ‘©â€ğŸ’» **Dipali Gangarde**

---

## ğŸ“„ License

Licensed under the [MIT License](LICENSE).

ğŸ•’ *Last updated: March 16, 2025*
