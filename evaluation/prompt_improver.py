import google.generativeai as genai
import os
from dotenv import load_dotenv
import json

# Load environment variables
load_dotenv()
GOOGLE_API_KEY = os.getenv("GEMINI_API_KEY")

# Configure Gemini API
genai.configure(api_key=GOOGLE_API_KEY)

class PromptImprover:
    """Uses Gemini to provide system prompt improvements to boost RAGAS scores"""
    
    def __init__(self):
        self.model = genai.GenerativeModel("gemini-1.5-flash")
    
    def get_prompt_improvements(self, question, current_system_prompt, model_response, 
                               reference_response, scores):
        """
        Generate system prompt improvements to boost RAGAS scores
        
        Args:
            question: Original question asked
            current_system_prompt: Current system prompt used
            model_response: Response generated by the model
            reference_response: Expected/reference response
            scores: Current RAGAS scores
            
        Returns:
            Dict with prompt improvement suggestions and estimated gains
        """
        try:
            # Format the prompt for Gemini
            prompt = self._create_improvement_prompt(
                question, current_system_prompt, model_response, 
                reference_response, scores
            )
            
            # Get recommendations from Gemini
            result = self.model.generate_content(prompt)
            
            # Parse the response
            return self._parse_improvement_response(result.text)
            
        except Exception as e:
            print(f"Error generating prompt improvements: {e}")
            return {
                "error": str(e),
                "fallback_suggestions": self._generate_fallback_suggestions(scores)
            }
    
    def _create_improvement_prompt(self, question, current_system_prompt, 
                                 model_response, reference_response, scores):
        """Create the prompt for Gemini"""
        metrics_str = "\n".join([f"- {metric}: {score}" for metric, score in scores.items()])
        
        prompt = f"""As a prompt engineering expert, improve this system prompt to get RAGAS scores closer to the reference response.

QUESTION:
{question}

CURRENT SYSTEM PROMPT:
{current_system_prompt}

MODEL RESPONSE:
{model_response}

EXPECTED REFERENCE RESPONSE:
{reference_response}

CURRENT RAGAS SCORES:
{metrics_str}

TASK:
1. Analyze the gaps between the model response and reference response
2. Identify specific issues in the system prompt that caused these gaps
3. Suggest 3-4 concrete improvements to the system prompt
4. Provide a completely rewritten system prompt
5. Estimate percentage improvement for each RAGAS metric

FORMAT YOUR RESPONSE AS JSON:
{{
  "analysis": "Brief analysis of current issues",
  "prompt_improvements": [
    {{
      "issue": "Specific issue identified",
      "suggestion": "Concrete improvement suggestion",
      "target_metrics": ["Metric1", "Metric2"],
      "estimated_gains": {{
        "Metric1": "+X%",
        "Metric2": "+Y%"
      }}
    }}
  ],
  "improved_system_prompt": "Complete rewritten system prompt",
  "overall_estimated_improvement": "Overall expected improvement"
}}
"""
        return prompt
    
    def _parse_improvement_response(self, response_text):
        """Parse the Gemini response to extract improvements"""
        try:
            # Try to extract JSON content
            import json
            import re
            
            # Look for JSON pattern in the response
            json_match = re.search(r'(\{.*\})', response_text, re.DOTALL)
            
            if json_match:
                json_str = json_match.group(1)
                improvements = json.loads(json_str)
                return improvements
            else:
                # If no JSON found, do best-effort parsing
                return {"analysis": response_text}
                
        except Exception as e:
            print(f"Error parsing improvement response: {e}")
            return {"analysis": response_text}
    
    def _generate_fallback_suggestions(self, scores):
        """Generate basic fallback suggestions if LLM call fails"""
        # Find lowest scoring metrics
        lowest_metric = min(
            [m for m in scores.keys() if m != "Overall"],
            key=lambda m: scores[m]
        )
        
        fallback = {
            "Relevance": "Add specific instructions to directly address the question using key terms from the query.",
            "Accuracy": "Add instructions to prioritize factual correctness and verify statements.",
            "Groundedness": "Add instructions to use similar terminology and organization as reference material.",
            "Completeness": "Add instructions to cover all key aspects of the topic comprehensively.",
            "BLEU": "Add instructions to use more identical phrasing from reference examples.",
            "ROUGE": "Add instructions to match key terminology and sequence of the reference."
        }
        
        return [
            {
                "issue": f"Low {lowest_metric} score ({scores[lowest_metric]})",
                "suggestion": fallback.get(lowest_metric, "Improve prompt specificity"),
                "target_metrics": [lowest_metric],
                "estimated_gains": {lowest_metric: "+10-15%"}
            },
            {
                "issue": "General prompt clarity",
                "suggestion": "Add explicit structure requirements for responses",
                "target_metrics": ["Overall"],
                "estimated_gains": {"Overall": "+5-10%"}
            }
        ]