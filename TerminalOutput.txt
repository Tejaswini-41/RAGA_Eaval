# 🚀 RAGA\_EVAL: LLM Response Evaluation Tool

![RAGA_EVAL](https://img.shields.io/badge/RAGA_EVAL-LLM%20Evaluation%20Tool-blue)

## 📋 Overview

RAGA\_EVAL is an advanced tool designed to evaluate and optimize responses from Large Language Models (LLMs) through iterative prompt improvement. It generates ground truth responses using Gemini and compares them with responses from models such as Mixtral, Llama, Qwen, and DeepSeek. The evaluation uses multiple metrics, including Relevance, Accuracy, Groundedness, Completeness, BLEU, and ROUGE.

## ✨ Key Innovations

- 🔄 **Iterative Prompt Improvement**: Automatically optimizes prompts across 3 iterations.
- 📊 **Comprehensive Metrics**: Multiple evaluation dimensions for thorough analysis.
- ⏳ **API Rate Limit Handling**: Intelligent backoff strategies to prevent quota exhaustion.
- 📈 **Performance Visualization**: Clear gain/loss tracking between iterations.

## 🔍 Features

- 🧠 **Ground Truth Generation**: Uses Gemini to generate reference responses.
- 🤖 **Multi-Model Evaluation**: Tests Mixtral, Llama, Qwen, and DeepSeek via Groq API.
- 📊 **Comprehensive Metrics**: Evaluates using Relevance, Accuracy, Groundedness, Completeness, BLEU, and ROUGE.
- 🔄 **Iteration Tracking**: Shows performance changes across multiple prompt iterations.
- 💡 **Smart Prompt Enhancement**: Automatically identifies weak areas and improves prompts.
- 📁 **Results Export**: Saves detailed performance data in organized CSV reports.

## 📂 Folder Structure

```
📁 RAGA_EVAL/
├── 📄 .gitignore
├── 🛠️ .env
├── 📖 README.md
├── 📦 requirements.txt
├── 📝 interactive_eval.py       # Main evaluation interface
├── 📜 questions.json            # Input questions for evaluation
├── 📂 evaluation/               # Evaluation logic
│   ├── 📊 dynamic_recom.py      # Dynamic recommendations based on metrics
│   ├── 🧪 evaluator.py          # Core evaluation functionality
│   ├── 📏 metrics.py            # Metrics computation
│   ├── 🤖 prompt_improver.py    # Improves prompts based on feedback
│   └── 📝 recommendation.py     # Generates recommendations using Gemini
├── 📂 models/                   # Model interfaces
│   ├── 🧠 gemini_model.py       # Gemini for ground truth and recommendations
│   ├── 🤖 groq_models.py        # Groq-based models
│   └── 🔧 model_factory.py      # Model initialization and selection
├── 📂 responses/                # Stores generated responses
├── 📂 improved_responses/       # Stores responses from improved prompts
├── 📂 prompts/                  # Stores improved prompt versions
└── 📂 results/                  # CSV exports of evaluation metrics
```

## 🛠️ Technology Stack

- **🐍 Programming**: Python 3.8+
- **🧠 LLMs Used**:
  - 📖 Ground Truth: Gemini
  - 🤖 Response Generation: Mixtral, Llama, Qwen, DeepSeek (via Groq API)
- **📊 Evaluation Metrics**:
  - ✅ **Relevance**: Response pertinence to question
  - 🎯 **Accuracy**: Factual correctness
  - 📚 **Groundedness**: Based on reference material
  - 📖 **Completeness**: Coverage of required information
  - 🔡 **BLEU/ROUGE**: Text similarity metrics
- **💾 Data Storage**: JSON and CSV for structured output

## 📥 Installation

### 🔽 Clone the repository:

```bash
git clone https://github.com/Tejaswini-41/RAGA_Eval.git
cd RAGA_Eval
```

### 🛠️ Create a virtual environment:

```bash
python -m venv venv
source venv/bin/activate  # On macOS/Linux
venv\Scripts\activate    # On Windows
```

### 📦 Install dependencies:

```bash
pip install -r requirements.txt
```

### 🔑 Set up environment variables:

Create a `.env` file and add your API keys:

```bash
GEMINI_API_KEY=your_gemini_api_key
GROQ_API_KEY=your_groq_api_key
```

## 🚦 Usage

1. **Prepare questions**: Update or create questions in `questions.json`
2. **Run the evaluation script**:

```bash
python interactive_eval.py
```

3. **Interactive menu options**:
   - 📌 Evaluate single question
   - 🔄 Evaluate with iterative improvement (3 iterations)
   - 📜 View all available questions
   - ❌ Quit
4. **Review results**:
   - 📊 Terminal output shows metrics comparison
   - 📂 CSV exports in `results/` directory
   - 📜 Improved prompts stored in `prompts/`

## 🔄 Current Development Status

✅ Core evaluation framework complete
✅ Multi-model comparison working
✅ Iterative prompt improvement implemented
✅ API rate limiting and backoff strategies added
✅ CSV export functionality added
✅ Performance summary with best iteration identification
🔜 Integration with RAG-based agent (next phase)

## 📊 Sample Results

The system generates comprehensive metrics tables showing performance across iterations:

```
───────────────────────────────────────────────────────────────────────────────────────────────
Model       Metric       Original   Iter 1   Gain 1%  Iter 2   Gain 2%  Iter 3   Gain 3%  Total Gain
───────────────────────────────────────────────────────────────────────────────────────────────
alibaba     Accuracy     0.900      0.950   ✅+5.6%   0.500   🚩-44.4%   0.950   ✅+5.6%   ✅ +5.6%
            BLEU        0.043      0.126   ✅+193.0% 0.128   ✅+197.7% 0.137   ✅+218.6% ✅ +218.6%
            Completeness 0.500      0.800   ✅+60.0%  0.500   +0.0%     0.800   ✅+60.0%  ✅ +60.0%
            Groundedness 0.200      0.200   +0.0%    0.500   ✅+150.0% 0.200   +0.0%     +0.0%
            Overall      0.531      0.575   ✅+8.3%   0.470   🚩-11.5%  0.573   ✅+7.9%   ✅ +7.9%
            ROUGE       0.214      0.107   🚩-50.0%  0.102   🚩-52.3%  0.106   🚩-50.5%  🚩 -50.5%
            Relevance   0.828      0.771   🚩-6.9%   0.734   🚩-11.4%  0.758   🚩-8.5%   🚩 -8.5%
```

## 🔮 Future Enhancements

- 🔍 Integration with RAG-based agent for code review
- 📈 Enhanced visualization of metrics using charts
- 🧪 Support for additional evaluation dimensions
- 🔄 Parallel processing for faster evaluation
- 🌐 Web interface for easier interaction

## 👥 Contributors

- 👩‍💻 **Tejaswini Durge**
- 👩‍💻 **Dipali Gangarde**

## 📄 License

📜 This project is licensed under the MIT License.

🕒 *Last updated: March 16, 2025*

