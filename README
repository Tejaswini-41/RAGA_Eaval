ğŸš€ RAGA_EVAL: LLM Response Evaluation Tool
<img alt="Status" src="https://img.shields.io/badge/status-beta-green">
<img alt="Python" src="https://img.shields.io/badge/python-3.8+-blue">
<img alt="License" src="https://img.shields.io/badge/license-MIT-orange">
ğŸ“‹ Overview
RAGA_EVAL is an advanced tool designed to evaluate and optimize responses from Large Language Models (LLMs) through iterative prompt improvement. It generates ground truth responses using Gemini and compares them with responses from models such as Mixtral, Llama, Qwen, and DeepSeek. The evaluation uses multiple metrics including Relevance, Accuracy, Groundedness, Completeness, BLEU, and ROUGE.

âœ¨ Key Innovations
Iterative Prompt Improvement: Automatically optimizes prompts across 3 iterations
Comprehensive Metrics: Multiple evaluation dimensions for thorough analysis
API Rate Limit Handling: Intelligent backoff strategies to prevent quota exhaustion
Performance Visualization: Clear gain/loss tracking between iterations
ğŸ” Features
ğŸ§  Ground Truth Generation: Uses Gemini to generate reference responses
ğŸ¤– Multi-Model Evaluation: Tests Mixtral, Llama, Qwen, and DeepSeek via Groq API
ğŸ“Š Comprehensive Metrics: Evaluates using Relevance, Accuracy, Groundedness, Completeness, BLEU, and ROUGE
ğŸ“ˆ Iteration Tracking: Shows performance changes across multiple prompt iterations
ğŸ’¡ Smart Prompt Enhancement: Automatically identifies weak areas and improves prompts
ğŸ“ Results Export: Saves detailed performance data in organized CSV reports
ğŸ“‚ Folder Structure
RAGA_EVAL/ 
â”œâ”€â”€ .gitignore 
â”œâ”€â”€ .env
â”œâ”€â”€ README.md 
â”œâ”€â”€ requirements.txt 
â”œâ”€â”€ interactive_eval.py       # Main evaluation interface
â”œâ”€â”€ questions.json            # Input questions for evaluation 
â”œâ”€â”€ evaluation/               # Evaluation logic
â”‚   â”œâ”€â”€ dynamic_recom.py      # Dynamic recommendations based on metrics
â”‚   â”œâ”€â”€ evaluator.py          # Core evaluation functionality
â”‚   â”œâ”€â”€ metrics.py            # Metrics computation
â”‚   â”œâ”€â”€ prompt_improver.py    # Improves prompts based on feedback
â”‚   â””â”€â”€ recommendation.py     # Generates recommendations using Gemini
â”œâ”€â”€ models/                   # Model interfaces
â”‚   â”œâ”€â”€ gemini_model.py       # Gemini for ground truth and recommendations
â”‚   â”œâ”€â”€ groq_models.py        # Groq-based models
â”‚   â””â”€â”€ model_factory.py      # Model initialization and selection
â”œâ”€â”€ responses/                # Stores generated responses
â”œâ”€â”€ improved_responses/       # Stores responses from improved prompts
â”œâ”€â”€ prompts/                  # Stores improved prompt versions
â””â”€â”€ results/                  # CSV exports of evaluation metrics

ğŸ› ï¸ Technology Stack
ğŸ Programming: Python 3.8+
ğŸ§  LLMs Used:
Ground Truth: Gemini
Response Generation: Mixtral, Llama, Qwen, DeepSeek (via Groq API)
ğŸ“Š Evaluation Metrics:
Relevance: Response pertinence to question
Accuracy: Factual correctness
Groundedness: Based on reference material
Completeness: Coverage of required information
BLEU/ROUGE: Text similarity metrics
ğŸ’¾ Data Storage: JSON and CSV for structured output
ğŸ“¥ Installation
Clone the repository:
git clone https://github.com/Tejaswini-41/RAGA_Eaval.git
cd RAGA_Eaval
Create a virtual environment:

Install dependencies:

Set up environment variables:

Create a .env file and add your API keys:
ğŸš¦ Usage
Prepare questions: Update or create questions in questions.json

Run the evaluation script:

Interactive menu options:

Evaluate single question
Evaluate with iterative improvement (3 iterations)
View all available questions
Quit
Review results:

Terminal output shows metrics comparison
CSV exports in results directory
Improved prompts stored in prompts
ğŸ”„ Current Development Status
âœ… Core evaluation framework complete
âœ… Multi-model comparison working
âœ… Iterative prompt improvement implemented
âœ… API rate limiting and backoff strategies added
âœ… CSV export functionality added
âœ… Performance summary with best iteration identification
ğŸ”œ Integration with RAG-based agent (next phase)
ğŸ“Š Sample Results
The system generates comprehensive metrics tables showing performance across iterations:

Model           Metric          Original    Iter 1      Gain 1%    Iter 2      Gain 2%    Iter 3      Gain 3%    Total Gain
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
deepseek        Accuracy        0.900       0.950      âœ…+5.6%     0.950      âœ…+5.6%     0.950      âœ…+5.6%     âœ… +5.6%
                Completeness    0.800       0.850      âœ…+6.3%     0.900      âœ…+12.5%    0.850      âœ…+6.3%     âœ… +6.3%
                Groundedness    0.750       0.850      âœ…+13.3%    0.850      âœ…+13.3%    0.900      âœ…+20.0%    âœ… +20.0%

ğŸ”® Future Enhancements
ğŸ” Integration with RAG-based agent for code review
ğŸ“ˆ Enhanced visualization of metrics using charts
ğŸ§ª Support for additional evaluation dimensions
ğŸ”„ Parallel processing for faster evaluation
ğŸŒ Web interface for easier interaction
ğŸ‘¥ Contributors
Tejaswini Durge
Dipali Gangarde
ğŸ“„ License
This project is licensed under the MIT License.

Last updated: March 16, 2025                
