# RAGA_EVAL: LLM Response Evaluation Tool (IN PROGRESS)

## Overview
RAGA_EVAL is a tool designed to evaluate responses from Large Language Models (LLMs) based on **RAG (Retrieval-Augmented Generation) assessment**. It generates **ground truth responses using Gemini** and compares them with responses from **Groq-based models** such as **Mixtral, Llama, Qwen, and DeepSeek**. The evaluation is conducted using **RAGAS scores**, and **recommendations are provided by Gemini** to improve these scores, including an updated **system prompt**.

## Features
- **Ground Truth Generation**: Uses **Gemini** to generate reference responses.
- **LLM Response Evaluation**: Generates responses from **Mixtral, Llama, Qwen, and DeepSeek**.
- **RAGAS Scoring**: Evaluates model responses based on **retrieval accuracy, faithfulness, and relevance**.
- **Recommendation System**: Uses **Gemini** to provide suggestions on how to improve **RAGAS scores**.
- **Updated System Prompt**: Generates an improved **system prompt** for better response quality.

## Folder Structure

RAGA_EVAL/ 
├── .gitignore 
├── .env
├── README.md 
├── requirements.txt 
├── interactive_eval.py 
├── questions.json # Input questions for evaluation 
├── evaluation/ │ 
      ├── dynamic_recom.py # Dynamic recommendations based on RAGAS scores │ 
      ├── evaluator.py # Evaluation logic using RAGAS │ 
      ├── metrics.py # Metrics computation │       
      ├── prompt_improver.py # Improves prompts based on evaluation feedback │       
      ├── recommendation.py # Generates recommendations using Gemini 
├── models/ │ 
      ├── gemini_model.py # Gemini model for generating ground truth and recommendations │ 
      ├── groq_models.py # Groq models (Mixtral, Llama, Qwen, DeepSeek) │       
      ├── model_factory.py # Model initialization and selection │ 
├── responses/ # Stores model-generated responses 
├── utils/ 
├── venv/ 
├── Version1/



## Technology Stack
- **Programming Language**: Python
- **LLMs Used**:
  - **Ground Truth**: Gemini
  - **Response Generation**: Mixtral, Llama, Qwen, DeepSeek (via Groq API)
- **Evaluation Metrics**:
  - **RAGAS**: Measures faithfulness, context relevance, and answer correctness.
- **Data Storage**: JSON for structured output.


## Installation
1. Clone the repository:
   ```bash
   git clone https://github.com/Tejaswini-41/RAGA_Eaval.git
   ```
2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
3. Set up environment variables:
   - Create a `.env` file and add your API keys:
     ```
     GROQ_API_KEY=your_groq_api_key
     GEMINI_API_KEY=your_gemini_api_key
     ```

## Usage
1. Prepare your input questions in `questions.json`.
2. Run the evaluation script:
   ```bash
   python interactive_eval.py
   ```

## Future Enhancements
- Integrate additional LLMs like Gemini.
- Implement automated RAG-based evaluation scoring.

## Contributors
- Tejaswini Durge
- Dipali Gangarde

## License
This project is licensed under the MIT License.

