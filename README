# # ğŸš€ RAGA\_EVAL: LLM Response Evaluation Tool

# <div align="center"> 
#   <img src="https://img.shields.io/badge/version-1.0.0-blue.svg?cacheSeconds=2592000" /> 
#   <img src="https://img.shields.io/badge/python-3.8+-green.svg" /> 
#   <img src="https://img.shields.io/badge/license-MIT-yellow.svg" /> 
# </div> 

# <p align="center"> 
#   <img width="180" src="https://i.imgur.com/pSOxq3J.png" alt="RAGA_EVAL Logo"/> 
# </p> 

## ğŸ“‹ Overview

RAGA\_EVAL is an advanced tool designed to evaluate and optimize responses from Large Language Models (LLMs) through iterative prompt improvement. It generates ground truth responses using Gemini and compares them with responses from models such as Mixtral, Llama, Qwen, and DeepSeek. The evaluation uses multiple metrics, including Relevance, Accuracy, Groundedness, Completeness, BLEU, and ROUGE.

## âœ¨ Key Innovations

- ğŸ”„ **Iterative Prompt Improvement**: Automatically optimizes prompts across 3 iterations.
- ğŸ“Š **Comprehensive Metrics**: Multiple evaluation dimensions for thorough analysis.
- â³ **API Rate Limit Handling**: Intelligent backoff strategies to prevent quota exhaustion.
- ğŸ“ˆ **Performance Visualization**: Clear gain/loss tracking between iterations.

## ğŸ” Features

- ğŸ§  **Ground Truth Generation**: Uses Gemini to generate reference responses.
- ğŸ¤– **Multi-Model Evaluation**: Tests Mixtral, Llama, Qwen, and DeepSeek via Groq API.
- ğŸ“Š **Comprehensive Metrics**: Evaluates using Relevance, Accuracy, Groundedness, Completeness, BLEU, and ROUGE.
- ğŸ”„ **Iteration Tracking**: Shows performance changes across multiple prompt iterations.
- ğŸ’¡ **Smart Prompt Enhancement**: Automatically identifies weak areas and improves prompts.
- ğŸ“ **Results Export**: Saves detailed performance data in organized CSV reports.

## ğŸ“‚ Folder Structure

```
ğŸ“ RAGA_EVAL/
â”œâ”€â”€ ğŸ“„ .gitignore
â”œâ”€â”€ ğŸ› ï¸ .env
â”œâ”€â”€ ğŸ“– README.md
â”œâ”€â”€ ğŸ“¦ requirements.txt
â”œâ”€â”€ ğŸ“ interactive_eval.py       # Main evaluation interface
â”œâ”€â”€ ğŸ“œ questions.json            # Input questions for evaluation
â”œâ”€â”€ ğŸ“‚ evaluation/               # Evaluation logic
â”‚   â”œâ”€â”€ ğŸ“Š dynamic_recom.py      # Dynamic recommendations based on metrics
â”‚   â”œâ”€â”€ ğŸ§ª evaluator.py          # Core evaluation functionality
â”‚   â”œâ”€â”€ ğŸ“ metrics.py            # Metrics computation
â”‚   â”œâ”€â”€ ğŸ¤– prompt_improver.py    # Improves prompts based on feedback
â”‚   â””â”€â”€ ğŸ“ recommendation.py     # Generates recommendations using Gemini
â”œâ”€â”€ ğŸ“‚ models/                   # Model interfaces
â”‚   â”œâ”€â”€ ğŸ§  gemini_model.py       # Gemini for ground truth and recommendations
â”‚   â”œâ”€â”€ ğŸ¤– groq_models.py        # Groq-based models
â”‚   â””â”€â”€ ğŸ”§ model_factory.py      # Model initialization and selection
|â”€â”€ ğŸ“‚ RAGA_Eaval/
â”œâ”€â”€ RAGBasedAgent/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ change_analyzer.py     # PR code change analysis
â”‚   â”œâ”€â”€ embedding_store.py     # ChromaDB vector storage
â”‚   â”œâ”€â”€ FreeLLM_Wrapper.py     # Free LLM API integration
â”‚   â”œâ”€â”€ GithubAuth.py          # GitHub API interactions
â”‚   â”œâ”€â”€ main.py
|   |â”€â”€ RAGBasedAgent.py             # Main entry point
â”‚   â”œâ”€â”€ review_evaluator.py    # Model evaluation with metrics
â”‚   â”œâ”€â”€ review_generator.py    # Review generation & formatting
â”‚   â””â”€â”€ similarity_query.py    # Similar PR search
|
â”œâ”€â”€ ğŸ“‚ responses/                # Stores generated responses
â”œâ”€â”€ ğŸ“‚ improved_responses/       # Stores responses from improved prompts
â”œâ”€â”€ ğŸ“‚ prompts/                  # Stores improved prompt versions
â””â”€â”€ ğŸ“‚ results/                  # CSV exports of evaluation metrics
```

## ğŸ› ï¸ Technology Stack

- **ğŸ Programming**: Python 3.8+
- **ğŸ§  LLMs Used**:
  - ğŸ“– Ground Truth: Gemini
  - ğŸ¤– Response Generation: Mixtral, Llama, Qwen, DeepSeek (via Groq API)
- **ğŸ“Š Evaluation Metrics**:
  - âœ… **Relevance**: Response pertinence to question
  - ğŸ¯ **Accuracy**: Factual correctness
  - ğŸ“š **Groundedness**: Based on reference material
  - ğŸ“– **Completeness**: Coverage of required information
  - ğŸ”¡ **BLEU/ROUGE**: Text similarity metrics
- **ğŸ’¾ Data Storage**: JSON and CSV for structured output

## ğŸ“¥ Installation

### ğŸ”½ Clone the repository:

```bash
git clone https://github.com/Tejaswini-41/RAGA_Eaval.git
cd RAGA_Eval
```

### ğŸ› ï¸ Create a virtual environment:

```bash
python -m venv venv
source venv/bin/activate  # On macOS/Linux
venv\Scripts\activate    # On Windows
```

### ğŸ“¦ Install dependencies:

```bash
pip install -r requirements.txt
```

### ğŸ”‘ Set up environment variables:

Create a `.env` file and add your API keys:

```bash
# Required API keys
GITHUB_ACCESS_TOKEN=your_github_token
GEMINI_API_KEY=your_gemini_api_key

# Optional API keys for additional models
GROQ_API_KEY=your_groq_api_key
ANTHROPIC_API_KEY=your_anthropic_api_key
```

## ğŸš¦ Usage

1. **Prepare questions**: Update or create questions in `questions.json`
2. **Run the evaluation script**:

```bash
python interactive_eval.py
```

3. **Interactive menu options**:
   - ğŸ“Œ Evaluate single question
   - ğŸ”„ Evaluate with iterative improvement (3 iterations)
   - ğŸ“œ View all available questions
   - âŒ Quit
4. **Review results**:
   - ğŸ“Š Terminal output shows metrics comparison
   - ğŸ“‚ CSV exports in `results/` directory
   - ğŸ“œ Improved prompts stored in `prompts/`

## ğŸ”„ Current Development Status

âœ… Core evaluation framework complete
âœ… Multi-model comparison working
âœ… Iterative prompt improvement implemented
âœ… API rate limiting and backoff strategies added
âœ… CSV export functionality added
âœ… Performance summary with best iteration identification
ğŸ”œ Integration with RAG-based agent (next phase)

## ğŸ“Š Sample Results

The system generates comprehensive metrics tables showing performance across iterations:

```
ğŸ§ª Step 6: Evaluating AI models for review quality...
â€¢ Generating reference review with gemini...
â€¢ Testing deepseek model...
  - Overall score: 0.589
â€¢ Testing llama model...
  - Overall score: 0.625
â€¢ Testing alibaba model...
  - Overall score: 0.485

âœ… Best model for PR review: llama (Score: 0.625)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š MODEL COMPARISON (Reference: GEMINI)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Model        | Overall | Relev | Accur | Grnd  | Comp  | Faith | Ctx   | Answ  | BLEU  | ROUG  |
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
llama        | 0.625   | 0.74  | 0.85  | 0.80  | 0.20  | 0.14  | 0.80  | 0.88  | 0.18  | 0.20  |
deepseek     | 0.589   | 0.77  | 0.90  | 0.60  | 0.20  | 0.50  | 0.00  | 0.86  | 0.04  | 0.17  |
alibaba      | 0.485   | 0.80  | 0.80  | 0.80  | 0.20  | 0.00  | 0.00  | 0.00  | 0.11  | 0.19  |
```

## ğŸ”® Future Enhancements

- ğŸ” Integration with RAG-based agent for code review
- ğŸ“ˆ Enhanced visualization of metrics using charts
- ğŸ§ª Support for additional evaluation dimensions
- ğŸ”„ Parallel processing for faster evaluation
- ğŸŒ Web interface for easier interaction

## ğŸ‘¥ Contributors

- ğŸ‘©â€ğŸ’» **Tejaswini Durge**
- ğŸ‘©â€ğŸ’» **Dipali Gangarde**

## ğŸ“„ License

ğŸ“œ This project is licensed under the MIT License.

ğŸ•’ *Last updated: March 16, 2025*

Last updated: March 16, 2025                